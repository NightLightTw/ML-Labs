{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb318ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Step 1: Unzip Dataset\n",
    "# ============================\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# TODO: Set the correct uploaded ZIP filename\n",
    "zip_path = 'dataset.zip' # <-- replace with the exact uploaded filename\n",
    "extract_path = './dataset' # folder where you want to extract\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "print(f\"✅ Unzipped to: {os.path.abspath(extract_path)}\")\n",
    "\n",
    "# Optional: List the extracted folder contents\n",
    "print(\"Contents:\")\n",
    "print(os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1866144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Step 2: Split Dataset into Train/Test\n",
    "# ============================\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# TODO: Set source folder (where unzipped dataset is) and target folder (where split dataset will go)\n",
    "SOURCE_DIR = 'dataset' # folder from unzip\n",
    "TARGET_DIR = 'dataset_split' # new folder to store split data\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.3 # note: you can calculate this as 1 - train_ratio if needed\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Create target train/test directories per class\n",
    "for split in ['train', 'test']:\n",
    "    for class_name in os.listdir(SOURCE_DIR):\n",
    "        os.makedirs(os.path.join(TARGET_DIR, split, class_name), exist_ok=True)\n",
    "        \n",
    "# Process each class folder\n",
    "for class_name in os.listdir(SOURCE_DIR):\n",
    "    class_path = os.path.join(SOURCE_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "        \n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "        \n",
    "    # Calculate split point\n",
    "    train_cutoff = int(len(images) * train_ratio)\n",
    "\n",
    "    # Split images\n",
    "    train_images = images[:train_cutoff]\n",
    "    test_images = images[train_cutoff:]\n",
    "\n",
    "    # Copy training images\n",
    "    for img_name in train_images:\n",
    "        src = os.path.join(class_path, img_name)\n",
    "        dst = os.path.join(TARGET_DIR, 'train', class_name, img_name)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # Copy testing images\n",
    "    for img_name in test_images:\n",
    "        src = os.path.join(class_path, img_name)\n",
    "        dst = os.path.join(TARGET_DIR, 'test', class_name, img_name)\n",
    "        shutil.copyfile(src, dst)\n",
    "print(\"✅ Dataset split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set dataset paths (update if needed)\n",
    "TRAIN_PATH = 'dataset_split/train'\n",
    "TEST_PATH = 'dataset_split/test'\n",
    "\n",
    "# Define image transforms: resize, grayscale, tensor, normalize\n",
    "transform_custom = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_set = torchvision.datasets.ImageFolder(root=TRAIN_PATH, transform=transform_custom)\n",
    "test_set = torchvision.datasets.ImageFolder(root=TEST_PATH, transform=transform_custom)\n",
    "\n",
    "# Print dataset info\n",
    "print(\"Classes:\", train_set.classes)\n",
    "print(\"Train samples:\", len(train_set))\n",
    "print(\"Test samples:\", len(test_set))\n",
    "\n",
    "# Show example images (2 per class)\n",
    "def show_2x6_grid(dataset, n_per_class=2, title=\"Example Grid\"):\n",
    "    class_counts = {i: 0 for i in range(len(dataset.classes))}\n",
    "    collected = {i: [] for i in range(len(dataset.classes))}\n",
    "        \n",
    "    for img, label in dataset:\n",
    "        if class_counts[label] < n_per_class:\n",
    "            collected[label].append(img)\n",
    "            class_counts[label] += 1\n",
    "        if all(c >= n_per_class for c in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    fig, axes = plt.subplots(n_per_class, len(dataset.classes), figsize=(len(dataset.classes)*2, n_per_class*2))\n",
    "    for col, imgs in collected.items():\n",
    "        for row in range(n_per_class):\n",
    "            ax = axes[row][col] if n_per_class > 1 else axes[col]\n",
    "            img = imgs[row].numpy().transpose(1, 2, 0) * 0.5 + 0.5 # unnormalize\n",
    "            ax.imshow(img.squeeze(), cmap='gray')\n",
    "            ax.set_title(dataset.classes[col], fontsize=8)\n",
    "            ax.axis('off')\n",
    "        \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show training and test grids\n",
    "show_2x6_grid(train_set, 2, \"Train Set Grid\")\n",
    "show_2x6_grid(test_set, 2, \"Test Set Grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6deb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "# === Helper ===\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "# === TODO 1: Define PreNorm block ===\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        # TODO: initialize LayerNorm and store fn\n",
    "\n",
    "    def forward(self, x,**kwargs):\n",
    "        # TODO: apply LayerNorm + fn\n",
    "        pass\n",
    "\n",
    "# === TODO 2: Define FeedForward block ===\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        # TODO: define two Linear layers + activation + dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: apply layers\n",
    "        pass\n",
    "\n",
    "# === TODO 3: Define Attention block ===\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        # TODO: set up qkv projections, softmax attention, final output projection\n",
    "        \n",
    "def forward(self, x):\n",
    "    # TODO: compute q, k, v, attention, and output\n",
    "    pass\n",
    "# === TODO 4: Define Transformer Encoder ===\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        # TODO: stack multiple PreNorm + Attention + FeedForward layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: pass through each Transformer layer\n",
    "        pass\n",
    "\n",
    "# === TODO 5: Define Vision Transformer (ViT) ===\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, channels, dim, depth, heads, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        # TODO: calculate patch numbers, set up patch embedding, positional embedding, cls token, transformer, mlp head\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # TODO: apply patch embedding, add cls token + pos embedding, run transformer, pool, mlp head\n",
    "        pass\n",
    "\n",
    "# === TODO 6: Initialize model ===\n",
    "# Example hyperparameters (students should adjust!)\n",
    "model = ViT(\n",
    "    image_size=28,\n",
    "    patch_size=4,\n",
    "    num_classes=6,\n",
    "    channels=1,\n",
    "    dim=64,\n",
    "    depth=6,\n",
    "    heads=4,\n",
    "    mlp_dim=128\n",
    ")\n",
    "\n",
    "# === TODO 7: Set up optimizer ===\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO 8: Print or summarize the model ===\n",
    "print(model)\n",
    "# Optionally: from torchsummary import summary\n",
    "# summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be966086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO 1: Define training epoch ===\n",
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    model.train()\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    \n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "    # TODO: Zero gradients\n",
    "    # TODO: Forward pass\n",
    "    # TODO: Compute loss\n",
    "    # TODO: Backward pass and optimizer step\n",
    "        if i % 100 == 0:\n",
    "        # TODO: Print progress info and save loss\n",
    "        pass\n",
    "\n",
    "# === TODO 2: Define evaluation function ===\n",
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            # TODO: Forward pass\n",
    "            # TODO: Compute loss\n",
    "            # TODO: Get predictions and count correct samples\n",
    "            \n",
    "    # TODO: Compute average loss and accuracy\n",
    "    # TODO: Print evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2394d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SET EPOCHS ===\n",
    "N_EPOCHS = 1\n",
    "\n",
    "# === START TIMER ===\n",
    "start_time = time.time()\n",
    "\n",
    "# === INIT LOSS TRACKERS ===\n",
    "train_loss_history, test_loss_history = [], []\n",
    "\n",
    "# === MAIN TRAINING LOOP ===\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "    evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "# === PRINT TOTAL TIME ===\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
    "\n",
    "# === SAVE TRAINED MODEL ===\n",
    "torch.save(model.state_dict(), 'Student_ID.pth') # replace with your actual Student ID\n",
    "print(\"✅ Model saved as .pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL - if needed\n",
    "# Make sure you define the same ViT model structure first\n",
    "model = ViT(image_size=28, patch_size=4, num_classes=6, channels=1, dim=64, depth=6, heads=4,\n",
    "mlp_dim=128)\n",
    "\n",
    "# Load saved weights\n",
    "model.load_state_dict(torch.load('Student_ID.pth'))\n",
    "model.eval()\n",
    "print(\"✅ Model loaded and ready for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# === TODO: Define function to plot confusion matrix ===\n",
    "def plot_confusion_matrix(model, data_loader, class_names):\n",
    "    # HINT:\n",
    "    # - Get predictions and true labels\n",
    "    # - Compute confusion matrix (sklearn)\n",
    "    # - Plot with seaborn heatmap\n",
    "    pass\n",
    "\n",
    "# === TODO: Define function to plot example predictions ===\n",
    "def plot_classwise_predictions(model, data_loader, class_names, samples_per_class=4):\n",
    "    # HINT:\n",
    "    # - Collect a few correct/incorrect predictions per class\n",
    "    # - Plot grid of images with true vs predicted labels\n",
    "    pass\n",
    "\n",
    "# === TODO: After training, call both functions ===\n",
    "# plot_confusion_matrix(model, test_loader, train_set.classes)\n",
    "# plot_classwise_predictions(model, test_loader, train_set.classes, samples_per_class=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
